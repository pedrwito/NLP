# Procesamiento de Lenguaje Natural (NLP) - CEIA

**Autor:** Pedro Lucas Barrera  
**Carrera:** Carrera de Especializaci칩n en Inteligencia Artificial (CEIA)  
**Materia:** Procesamiento de Lenguaje Natural  

Este repositorio contiene todos los desaf칤os y trabajos pr치cticos desarrollados durante el curso de NLP de la CEIA. Cada notebook implementa diferentes t칠cnicas y algoritmos fundamentales del procesamiento de lenguaje natural.

## 游늬 Estructura del Repositorio

### 游늵 Notebooks de Desaf칤os

#### `desafio1_pedro_barrera.ipynb`
**Tema:** Vectorizaci칩n de Documentos y Clasificaci칩n con Na칦ve Bayes

**Descripci칩n:**
Este notebook implementa t칠cnicas de vectorizaci칩n de texto y clasificaci칩n utilizando el dataset 20newsgroups. Se desarrollan tres puntos principales:

1. **An치lisis de Similitud de Documentos:** Vectorizaci칩n TF-IDF de documentos, selecci칩n aleatoria de 5 documentos y an치lisis de sus 5 m치s similares usando similitud coseno.

2. **Optimizaci칩n de Clasificadores Na칦ve Bayes:** Entrenamiento y optimizaci칩n de modelos MultinomialNB y ComplementNB, probando diferentes configuraciones de vectorizadores y hiperpar치metros para maximizar el F1-score macro.

3. **Similitud entre Palabras:** Transposici칩n de la matriz documento-t칠rmino para obtener vectores de palabras y an치lisis de similitud sem치ntica entre t칠rminos seleccionados manualmente.

**Resultados destacados:**
- Mejor configuraci칩n: ComplementNB con alpha=0.1 y vectorizador TF-IDF default (F1-score: 0.6954)
- An치lisis detallado de similitudes entre palabras como 'computer', 'baseball', 'medical', 'religion' y 'car'

#### `desafio2_pedro_barrera.ipynb`
**Tema:** Word Embeddings con Gensim y Word2Vec

**Descripci칩n:**
Implementaci칩n de word embeddings utilizando Gensim sobre un corpus personalizado de letras de canciones de Bob Dylan. El notebook incluye:

1. **Entrenamiento de Word2Vec:** Configuraci칩n y entrenamiento de un modelo Skip-gram con 300 dimensiones, ventana de contexto de 2 palabras y 20 negative samples.

2. **An치lisis de Similitudes Sem치nticas:** Exploraci칩n de palabras similares para t칠rminos como "love", "night", "woman", "man", "truth" y "heart", analizando las relaciones sem치nticas capturadas por el modelo.

3. **Operaciones Vectoriales:** Pruebas de combinaciones de vectores (ej: woman + love + man) para explorar relaciones sem치nticas complejas.

4. **Visualizaci칩n:** Mapas de calor de similitudes entre t칠rminos seleccionados y visualizaciones t-SNE en 2D y 3D de los embeddings.

**Resultados destacados:**
- Vocabulario de 948 palabras 칰nicas del corpus de Bob Dylan
- An치lisis de patrones po칠ticos y emocionales en las similitudes sem치nticas
- Visualizaciones interactivas de espacios de embeddings

#### `desafio3.ipynb`
**Tema:** Modelos de Lenguaje con Redes Neuronales Recurrentes

**Descripci칩n:**
Este desaf칤o explora el desarrollo de modelos de lenguaje utilizando capas LSTM para generar texto bas치ndose en "La Metamorfosis" de Kafka. El notebook demuestra c칩mo los modelos pueden aprender patrones del lenguaje espa침ol:

1. **Preprocesamiento del Corpus:** Se trabaja con el texto completo de "La Metamorfosis", realizando limpieza, normalizaci칩n de texto y an치lisis de distribuci칩n de longitudes de secuencias para determinar el contexto 칩ptimo del modelo.

2. **Integraci칩n con FastText:** Se integran embeddings pre-entrenados de FastText en espa침ol (300 dimensiones). El modelo descarga autom치ticamente los embeddings si no est치n presentes, lo que mejora significativamente la representaci칩n sem치ntica de las palabras.

3. **Arquitectura del Modelo:** Se dise침a una red neuronal secuencial con:
   - Capa de Embedding usando FastText (congelada, no entrenable)
   - Dos capas LSTM de 64 unidades cada una con return_sequences=True
   - Capa de Dropout (0.2) para regularizaci칩n
   - Capa Dense final con activaci칩n softmax para predicci칩n de siguiente palabra

4. **Entrenamiento con Perplejidad:** Se implementa un callback personalizado (PplCallback) para monitorear la perplejidad durante el entrenamiento, incluyendo early stopping cuando la m칠trica no mejora por varias 칠pocas consecutivas.

5. **Estrategias de Generaci칩n:** Se desarrollan diferentes t칠cnicas de generaci칩n de texto:
   - Generaci칩n greedy (selecci칩n determin칤stica)
   - Beam search determin칤stico
   - Beam search estoc치stico con control de temperatura
   - Interface interactiva con Gradio para experimentar en tiempo real

**Resultados obtenidos:**
- Modelo LSTM de dos capas (64 unidades cada una) entrenado durante 20 칠pocas
- Sistema de beam search configurable que permite controlar la creatividad vs coherencia
- Interface Gradio funcional para generar texto a partir de cualquier entrada
- Mejoras notables en la calidad del texto generado gracias a los embeddings FastText

**Observaciones:**
Aunque el modelo logra generar texto, produce secuencias que no tienen mucho sentido sem치ntico. Esto es t칤pico de los modelos de lenguaje m치s b치sicos y demuestra la complejidad real del procesamiento de lenguaje natural.

#### `desafio4_pedro_barrera.ipynb`
**Tema:** Chatbot QA con Arquitectura Encoder-Decoder y FastText Embeddings

**Descripci칩n:**
Desarrollo de un chatbot conversacional usando el dataset ConvAI2 (Conversational Intelligence Challenge 2) con una arquitectura encoder-decoder basada en LSTM. Este desaf칤o representa el proyecto m치s ambicioso de la materia, combinando m칰ltiples t칠cnicas avanzadas de NLP:

1. **Procesamiento del Dataset ConvAI2:** Descarga y preprocesamiento de 6,033 pares pregunta-respuesta extra칤dos de conversaciones reales en ingl칠s, implementando limpieza de texto y tokenizaci칩n especializada para di치logos.

2. **Integraci칩n de FastText Embeddings:** Implementaci칩n de una clase personalizada para cargar y manejar embeddings pre-entrenados FastText (cc.en.300.vec) de 1.6GB, optimizando el almacenamiento con serializaci칩n pickle para cargas futuras m치s r치pidas.

3. **Arquitectura Encoder-Decoder Avanzada:** Dise침o e implementaci칩n de un modelo seq2seq con:
   - Encoder LSTM (128 unidades) con embeddings FastText congelados
   - Decoder LSTM con embeddings entrenables
   - Tokens especiales `<sos>` y `<eos>` para control de secuencias
   - Arquitecturas separadas para entrenamiento e inferencia

4. **Sistema de Inferencia Conversacional:** Desarrollo de una funci칩n de respuesta que simula conversaci칩n real, procesando entrada del usuario y generando respuestas palabra por palabra usando los modelos encoder y decoder por separado.

**Resultados obtenidos:**
- Modelo entrenado por 40 칠pocas con accuracy de validaci칩n del 73% y entrenamiento del 81%
- Chatbot funcional capaz de responder preguntas b치sicas en ingl칠s
- Sistema de embeddings optimizado que mejora significativamente la representaci칩n sem치ntica

**Desaf칤os enfrentados:**
Como era de esperarse en un proyecto tan complejo, el modelo presenta algunas limitaciones t칤picas de chatbots seq2seq b치sicos: tendencia a generar respuestas repetitivas como "i like to go to the beach" o "i am a student". Esto refleja la naturaleza del dataset de entrenamiento y las limitaciones inherentes de los modelos encoder-decoder sin mecanismos de atenci칩n, una experiencia valiosa que demuestra tanto las posibilidades como los l칤mites de estas arquitecturas.

### 游늯 Archivos de Datos

#### `la_metamorfosis.txt`
Texto completo de "La Metamorfosis" de Franz Kafka en espa침ol, utilizado como corpus de referencia para el desaf칤o 3.

#### `songs_dataset.zip`
Archivo comprimido que contiene el datasets con letras de canciones de distintos autores y bandas, uncluyendo el canciones de Bob Dylan (`bob-dylan.txt`) utilizado en el desaf칤o 2 para entrenar el modelo Word2Vec. El corpus incluye una colecci칩n de letras de canciones que permite explorar las relaciones sem치nticas y patrones po칠ticos caracter칤sticos del artista.

## 游 Instrucciones de Ejecuci칩n

### Prerrequisitos
```bash
# Instalar dependencias principales
pip install numpy pandas matplotlib seaborn scikit-learn
pip install tensorflow keras
pip install gensim
pip install gradio
pip install plotly
pip install scipy
pip install gdown  # Para descargas autom치ticas (Desaf칤o 4)
```

### Ejecuci칩n de los Notebooks

1. **Para el Desaf칤o 1:**
   ```bash
   jupyter notebook desafio1_pedro_barrera.ipynb
   ```
   - No requiere datasets externos (usa 20newsgroups de sklearn)

2. **Para el Desaf칤o 2:**
   ```bash
   jupyter notebook desafio2_pedro_barrera.ipynb
   ```
   - Requiere el dataset `songs_dataset/bob-dylan.txt`, que se encuentra comprimido en el repo.

3. **Para el Desaf칤o 3:**
   ```bash
   jupyter notebook desafio3.ipynb
   ```
   - Descarga autom치ticamente el dataset de canciones si no est치 presente
   - Tiempo estimado de entrenamiento: 30-45 minutos (dependiendo del hardware)
   - Incluye interface interactiva con Gradio

4. **Para el Desaf칤o 4:**
   ```bash
   jupyter notebook desafio4_pedro_barrera.ipynb
   ```
   - Descarga autom치ticamente el dataset ConvAI2 (2.58MB) y los embeddings FastText (1.6GB)
   - **Tiempo estimado total: 1-2 horas** (incluyendo descarga de embeddings y entrenamiento)
   - Requiere conexi칩n a internet para descargas autom치ticas
   - El modelo entrenado permite interacci칩n conversacional b치sica

### Configuraci칩n del Entorno

```bash
# Clonar el repositorio
git clone [URL_del_repositorio]
cd NLP

# Crear entorno virtual (recomendado)
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt  # Si existe
# O instalar manualmente las librer칤as listadas arriba
```

## 游늳 Conceptos Implementados

### T칠cnicas de NLP Cubiertas:
-  Vectorizaci칩n TF-IDF
-  Similitud Coseno
-  Tokenizaci칩n
-  Clasificaci칩n con Na칦ve Bayes (Multinomial y Complement)
-  Word Embeddings (Word2Vec y FastText)
-  Modelos de Lenguaje con LSTM
-  Arquitecturas Encoder-Decoder (Seq2Seq)
-  Generaci칩n de Texto (Greedy, Beam Search)
-  Chatbots Conversacionales

## 游닇 Licencia

Este material es parte del trabajo acad칠mico para la CEIA y est치 destinado 칰nicamente a fines educativos. Sin embargo el contenido es libre para ser utilizado para cualquier fin que se desee

---

Para cualquier consulta o problema con la ejecuci칩n de los notebooks, revisar los comentarios dentro de cada archivo y si aun hay problemas no duden en contactarme :) 