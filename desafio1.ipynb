{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio 1 - Pedro Lucas Barrera a1801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 20newsgroups por ser un dataset clásico de NLP ya viene incluido y formateado\n",
    "# en sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los datos (ya separados de forma predeterminada en train y test)\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciamos un vectorizador\n",
    "# ver diferentes parámetros de instanciación en la documentación de sklearn https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "tfidfvect = TfidfVectorizer()\n",
    "X_train = tfidfvect.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 1: \n",
    " Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos. Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido la similaridad según el contenido del texto y la etiqueta de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tomamos 5 documentos al azar\n",
    "np.random.seed(42)  # Para reproducibilidad\n",
    "indices = np.random.choice(X_train.shape[0], 5, replace=False)\n",
    "random_items = X_train[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de documentos en el dataset: 11314\n",
      "Dimensionalidad del vector TF-IDF: 101631\n",
      "Índices de documentos seleccionados: [7492 3546 5582 4793 3813]\n",
      "\n",
      "DOCUMENTO 1 (numero: 7492)\n",
      "Etiqueta del documento a comprar: comp.sys.mac.hardware\n",
      "LOS 5 DOCUMENTOS MÁS SIMILARES:\n",
      "### 1°: Documento numero 10935\n",
      "- Similaridad: 0.6665\n",
      "- Etiqueta: comp.sys.mac.hardware\n",
      "### 2°: Documento numero 7258\n",
      "- Similaridad: 0.3476\n",
      "- Etiqueta: comp.sys.ibm.pc.hardware\n",
      "### 3°: Documento numero 4971\n",
      "- Similaridad: 0.1799\n",
      "- Etiqueta: comp.sys.mac.hardware\n",
      "### 4°: Documento numero 4303\n",
      "- Similaridad: 0.1547\n",
      "- Etiqueta: misc.forsale\n",
      "### 5°: Documento numero 645\n",
      "- Similaridad: 0.1414\n",
      "- Etiqueta: comp.sys.mac.hardware\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "DOCUMENTO 2 (numero: 3546)\n",
      "Etiqueta del documento a comprar: comp.os.ms-windows.misc\n",
      "LOS 5 DOCUMENTOS MÁS SIMILARES:\n",
      "### 1°: Documento numero 5665\n",
      "- Similaridad: 0.2040\n",
      "- Etiqueta: comp.sys.ibm.pc.hardware\n",
      "### 2°: Documento numero 2011\n",
      "- Similaridad: 0.1924\n",
      "- Etiqueta: comp.sys.ibm.pc.hardware\n",
      "### 3°: Documento numero 8643\n",
      "- Similaridad: 0.1724\n",
      "- Etiqueta: comp.sys.ibm.pc.hardware\n",
      "### 4°: Documento numero 1546\n",
      "- Similaridad: 0.1709\n",
      "- Etiqueta: comp.sys.ibm.pc.hardware\n",
      "### 5°: Documento numero 8765\n",
      "- Similaridad: 0.1616\n",
      "- Etiqueta: comp.sys.ibm.pc.hardware\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "DOCUMENTO 3 (numero: 5582)\n",
      "Etiqueta del documento a comprar: misc.forsale\n",
      "LOS 5 DOCUMENTOS MÁS SIMILARES:\n",
      "### 1°: Documento numero 5510\n",
      "- Similaridad: 0.4622\n",
      "- Etiqueta: misc.forsale\n",
      "### 2°: Documento numero 4922\n",
      "- Similaridad: 0.2999\n",
      "- Etiqueta: misc.forsale\n",
      "### 3°: Documento numero 4347\n",
      "- Similaridad: 0.2740\n",
      "- Etiqueta: comp.graphics\n",
      "### 4°: Documento numero 8057\n",
      "- Similaridad: 0.2076\n",
      "- Etiqueta: misc.forsale\n",
      "### 5°: Documento numero 4028\n",
      "- Similaridad: 0.1685\n",
      "- Etiqueta: comp.graphics\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "DOCUMENTO 4 (numero: 4793)\n",
      "Etiqueta del documento a comprar: talk.politics.guns\n",
      "LOS 5 DOCUMENTOS MÁS SIMILARES:\n",
      "### 1°: Documento numero 6894\n",
      "- Similaridad: 0.2364\n",
      "- Etiqueta: talk.politics.guns\n",
      "### 2°: Documento numero 5856\n",
      "- Similaridad: 0.2363\n",
      "- Etiqueta: sci.crypt\n",
      "### 3°: Documento numero 4271\n",
      "- Similaridad: 0.2328\n",
      "- Etiqueta: talk.politics.misc\n",
      "### 4°: Documento numero 3141\n",
      "- Similaridad: 0.2295\n",
      "- Etiqueta: talk.politics.guns\n",
      "### 5°: Documento numero 10836\n",
      "- Similaridad: 0.2291\n",
      "- Etiqueta: alt.atheism\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "DOCUMENTO 5 (numero: 3813)\n",
      "Etiqueta del documento a comprar: rec.sport.hockey\n",
      "LOS 5 DOCUMENTOS MÁS SIMILARES:\n",
      "### 1°: Documento numero 10836\n",
      "- Similaridad: 0.2514\n",
      "- Etiqueta: alt.atheism\n",
      "### 2°: Documento numero 759\n",
      "- Similaridad: 0.2480\n",
      "- Etiqueta: soc.religion.christian\n",
      "### 3°: Documento numero 913\n",
      "- Similaridad: 0.2410\n",
      "- Etiqueta: alt.atheism\n",
      "### 4°: Documento numero 5826\n",
      "- Similaridad: 0.2409\n",
      "- Etiqueta: soc.religion.christian\n",
      "### 5°: Documento numero 5856\n",
      "- Similaridad: 0.2329\n",
      "- Etiqueta: sci.crypt\n",
      "\n",
      "------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total de documentos en el dataset: {X_train.shape[0]}\")\n",
    "print(f\"Dimensionalidad del vector TF-IDF: {X_train.shape[1]}\")\n",
    "print(f\"Índices de documentos seleccionados: {indices}\\n\")\n",
    "\n",
    "# Para cada documento seleccionado, busco los 5 más similares\n",
    "indexes_most_similar_documents = {}\n",
    "for i, doc_index in enumerate(indices):\n",
    "    print(f\"DOCUMENTO {i+1} (numero: {doc_index})\")\n",
    "    \n",
    "    \n",
    "    current_doc = X_train[doc_index]\n",
    "    similarities = cosine_similarity(current_doc, X_train).flatten()\n",
    "    \n",
    "    #Tomamos 6 elementos con mayor simulitud, dado que el primero sera el mismo documento (con similaridad = 1.0), y lo removemos el primero.\n",
    "    most_similar_indices = similarities.argsort()[-6:][::-1]\n",
    "    most_similar_indices = most_similar_indices[1:]\n",
    "    indexes_most_similar_documents[i] = most_similar_indices\n",
    "    \n",
    "    print(f\"Etiqueta del documento a comprar: {newsgroups_train.target_names[newsgroups_train.target[doc_index]]}\")\n",
    "    print(\"LOS 5 DOCUMENTOS MÁS SIMILARES:\")\n",
    "    \n",
    "    for rank, similar_index in enumerate(most_similar_indices, 1):\n",
    "        similarity_score = similarities[similar_index]\n",
    "        similar_label = newsgroups_train.target_names[newsgroups_train.target[similar_index]]\n",
    "        \n",
    "        print(f\"### {rank}°: Documento numero {similar_index}\")\n",
    "        print(f\"- Similaridad: {similarity_score:.4f}\")\n",
    "        print(f\"- Etiqueta: {similar_label}\")\n",
    "    print(\"\\n------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizo la similitud tomando en cuenta su titulo y contenido:\n",
    "- Documento1: \n",
    "    Casi todos los titulos de los documentos mas similares son de computacion, al igual con el que se lo compara. Sin embargo, hay uno que es 'misc.forsale', osea, miscelaneo a la venta. Viendo el contenido del mismo, puede verse que...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimo los documentos mas similares por cada uno de los elegidos al azar y los comparo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 2: \n",
    "  Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación (f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial y ComplementNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del conjunto de entrenamiento: (11314, 101631)\n",
      "Forma del conjunto de test: (7532, 101631)\n",
      "Número de clases: 20\n",
      "Clases: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test = tfidfvect.transform(newsgroups_test.data)\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "print(f\"Forma del conjunto de entrenamiento: {X_train.shape}\")\n",
    "print(f\"Forma del conjunto de test: {X_test.shape}\")\n",
    "print(f\"Número de clases: {len(newsgroups_train.target_names)}\")\n",
    "print(f\"Clases: {newsgroups_train.target_names}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUACIÓN CON VECTORIZADOR BÁSICO TF-IDF:\n",
      "--------------------------------------------------\n",
      "MultinomialNB (básico): F1-score macro = 0.5854\n",
      "ComplementNB (básico): F1-score macro = 0.6930\n"
     ]
    }
   ],
   "source": [
    "# Función para evaluar un modelo\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    print(f\"{model_name}: F1-score macro = {f1:.4f}\")\n",
    "    return f1, y_pred\n",
    "\n",
    "print(\"EVALUACIÓN CON VECTORIZADOR BÁSICO TF-IDF:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Evaluación inicial con MultinomialNB\n",
    "multinomial_nb = MultinomialNB()\n",
    "f1_multi_basic, _ = evaluate_model(multinomial_nb, X_train, y_train, X_test, y_test, \"MultinomialNB (básico)\")\n",
    "\n",
    "# Evaluación inicial con ComplementNB\n",
    "complement_nb = ComplementNB()\n",
    "f1_comp_basic, _ = evaluate_model(complement_nb, X_train, y_train, X_test, y_test, \"ComplementNB (básico)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZACIÓN DE PARÁMETROS DEL VECTORIZADOR:\n",
      "--------------------------------------------------\n",
      "\n",
      "Probando vectorizador: TF-IDF básico\n",
      "----------------------------------------\n",
      "Dimensionalidad: 101631 características\n",
      "MultinomialNB (alpha=1.0): F1-score macro = 0.5854\n",
      "MultinomialNB (alpha=0.1): F1-score macro = 0.6565\n",
      "MultinomialNB (alpha=0.01): F1-score macro = 0.6829\n",
      "ComplementNB (alpha=1.0): F1-score macro = 0.6930\n",
      "ComplementNB (alpha=0.1): F1-score macro = 0.6954\n",
      "ComplementNB (alpha=0.01): F1-score macro = 0.6689\n",
      "\n",
      "Probando vectorizador: TF-IDF con min_df=2\n",
      "----------------------------------------\n",
      "Dimensionalidad: 39423 características\n",
      "MultinomialNB (alpha=1.0): F1-score macro = 0.5970\n",
      "MultinomialNB (alpha=0.1): F1-score macro = 0.6727\n",
      "MultinomialNB (alpha=0.01): F1-score macro = 0.6809\n",
      "ComplementNB (alpha=1.0): F1-score macro = 0.6935\n",
      "ComplementNB (alpha=0.1): F1-score macro = 0.6906\n",
      "ComplementNB (alpha=0.01): F1-score macro = 0.6747\n",
      "\n",
      "Probando vectorizador: TF-IDF con max_df=0.8\n",
      "----------------------------------------\n",
      "Dimensionalidad: 101630 características\n",
      "MultinomialNB (alpha=1.0): F1-score macro = 0.5884\n",
      "MultinomialNB (alpha=0.1): F1-score macro = 0.6578\n",
      "MultinomialNB (alpha=0.01): F1-score macro = 0.6831\n",
      "ComplementNB (alpha=1.0): F1-score macro = 0.6920\n",
      "ComplementNB (alpha=0.1): F1-score macro = 0.6950\n",
      "ComplementNB (alpha=0.01): F1-score macro = 0.6692\n",
      "\n",
      "Probando vectorizador: TF-IDF con min_df=2, max_df=0.8\n",
      "----------------------------------------\n",
      "Dimensionalidad: 39422 características\n",
      "MultinomialNB (alpha=1.0): F1-score macro = 0.6010\n",
      "MultinomialNB (alpha=0.1): F1-score macro = 0.6733\n",
      "MultinomialNB (alpha=0.01): F1-score macro = 0.6809\n",
      "ComplementNB (alpha=1.0): F1-score macro = 0.6925\n",
      "ComplementNB (alpha=0.1): F1-score macro = 0.6907\n",
      "ComplementNB (alpha=0.01): F1-score macro = 0.6741\n",
      "\n",
      "Probando vectorizador: TF-IDF con max_features=10000\n",
      "----------------------------------------\n",
      "Dimensionalidad: 10000 características\n",
      "MultinomialNB (alpha=1.0): F1-score macro = 0.6153\n",
      "MultinomialNB (alpha=0.1): F1-score macro = 0.6600\n",
      "MultinomialNB (alpha=0.01): F1-score macro = 0.6535\n",
      "ComplementNB (alpha=1.0): F1-score macro = 0.6672\n",
      "ComplementNB (alpha=0.1): F1-score macro = 0.6607\n",
      "ComplementNB (alpha=0.01): F1-score macro = 0.6565\n",
      "\n",
      "Probando vectorizador: TF-IDF con ngram_range=(1,2)\n",
      "----------------------------------------\n",
      "Dimensionalidad: 20000 características\n",
      "MultinomialNB (alpha=1.0): F1-score macro = 0.5866\n",
      "MultinomialNB (alpha=0.1): F1-score macro = 0.6411\n",
      "MultinomialNB (alpha=0.01): F1-score macro = 0.6348\n",
      "ComplementNB (alpha=1.0): F1-score macro = 0.6646\n",
      "ComplementNB (alpha=0.1): F1-score macro = 0.6629\n",
      "ComplementNB (alpha=0.01): F1-score macro = 0.6608\n",
      "\n",
      "Probando vectorizador: TF-IDF con ngram_range=(1,2), min_df=2, max_df=0.8\n",
      "----------------------------------------\n",
      "Dimensionalidad: 15000 características\n",
      "MultinomialNB (alpha=1.0): F1-score macro = 0.5860\n",
      "MultinomialNB (alpha=0.1): F1-score macro = 0.6295\n",
      "MultinomialNB (alpha=0.01): F1-score macro = 0.6190\n",
      "ComplementNB (alpha=1.0): F1-score macro = 0.6509\n",
      "ComplementNB (alpha=0.1): F1-score macro = 0.6504\n",
      "ComplementNB (alpha=0.01): F1-score macro = 0.6494\n",
      "\n",
      "Probando vectorizador: TF-IDF con stop_words=english\n",
      "----------------------------------------\n",
      "Dimensionalidad: 101322 características\n",
      "MultinomialNB (alpha=1.0): F1-score macro = 0.6468\n",
      "MultinomialNB (alpha=0.1): F1-score macro = 0.6726\n",
      "MultinomialNB (alpha=0.01): F1-score macro = 0.6844\n",
      "ComplementNB (alpha=1.0): F1-score macro = 0.6936\n",
      "ComplementNB (alpha=0.1): F1-score macro = 0.6919\n",
      "ComplementNB (alpha=0.01): F1-score macro = 0.6652\n",
      "\n",
      "Probando vectorizador: TF-IDF optimizado\n",
      "----------------------------------------\n",
      "Dimensionalidad: 12000 características\n",
      "MultinomialNB (alpha=1.0): F1-score macro = 0.6463\n",
      "MultinomialNB (alpha=0.1): F1-score macro = 0.6664\n",
      "MultinomialNB (alpha=0.01): F1-score macro = 0.6563\n",
      "ComplementNB (alpha=1.0): F1-score macro = 0.6730\n",
      "ComplementNB (alpha=0.1): F1-score macro = 0.6674\n",
      "ComplementNB (alpha=0.01): F1-score macro = 0.6637\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZACIÓN DE PARÁMETROS DEL VECTORIZADOR\n",
    "print(\"OPTIMIZACIÓN DE PARÁMETROS DEL VECTORIZADOR:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Lista de configuraciones de vectorizadores para probar\n",
    "vectorizer_configs = [\n",
    "    {\n",
    "        'name': 'TF-IDF básico',\n",
    "        'params': {}\n",
    "    },\n",
    "    {\n",
    "        'name': 'TF-IDF con min_df=2',\n",
    "        'params': {'min_df': 2}\n",
    "    },\n",
    "    {\n",
    "        'name': 'TF-IDF con max_df=0.8',\n",
    "        'params': {'max_df': 0.8}\n",
    "    },\n",
    "    {\n",
    "        'name': 'TF-IDF con min_df=2, max_df=0.8',\n",
    "        'params': {'min_df': 2, 'max_df': 0.8}\n",
    "    },\n",
    "    {\n",
    "        'name': 'TF-IDF con max_features=10000',\n",
    "        'params': {'max_features': 10000}\n",
    "    },\n",
    "    {\n",
    "        'name': 'TF-IDF con ngram_range=(1,2)',\n",
    "        'params': {'ngram_range': (1, 2), 'max_features': 20000}\n",
    "    },\n",
    "    {\n",
    "        'name': 'TF-IDF con ngram_range=(1,2), min_df=2, max_df=0.8',\n",
    "        'params': {'ngram_range': (1, 2), 'min_df': 2, 'max_df': 0.8, 'max_features': 15000}\n",
    "    },\n",
    "    {\n",
    "        'name': 'TF-IDF con stop_words=english',\n",
    "        'params': {'stop_words': 'english'}\n",
    "    },\n",
    "    {\n",
    "        'name': 'TF-IDF optimizado',\n",
    "        'params': {'min_df': 3, 'max_df': 0.7, 'ngram_range': (1, 2), 'max_features': 12000, 'stop_words': 'english'}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Lista de configuraciones de modelos para probar\n",
    "model_configs = [\n",
    "    {\n",
    "        'name': 'MultinomialNB (alpha=1.0)',\n",
    "        'model': MultinomialNB(alpha=1.0)\n",
    "    },\n",
    "    {\n",
    "        'name': 'MultinomialNB (alpha=0.1)',\n",
    "        'model': MultinomialNB(alpha=0.1)\n",
    "    },\n",
    "    {\n",
    "        'name': 'MultinomialNB (alpha=0.01)',\n",
    "        'model': MultinomialNB(alpha=0.01)\n",
    "    },\n",
    "    {\n",
    "        'name': 'ComplementNB (alpha=1.0)',\n",
    "        'model': ComplementNB(alpha=1.0)\n",
    "    },\n",
    "    {\n",
    "        'name': 'ComplementNB (alpha=0.1)',\n",
    "        'model': ComplementNB(alpha=0.1)\n",
    "    },\n",
    "    {\n",
    "        'name': 'ComplementNB (alpha=0.01)',\n",
    "        'model': ComplementNB(alpha=0.01)\n",
    "    }\n",
    "]\n",
    "\n",
    "best_f1 = 0\n",
    "best_config = None\n",
    "results = []\n",
    "\n",
    "# Probar todas las combinaciones\n",
    "for vec_config in vectorizer_configs:\n",
    "    print(f\"\\nProbando vectorizador: {vec_config['name']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Crear y entrenar el vectorizador\n",
    "    vectorizer = TfidfVectorizer(**vec_config['params'])\n",
    "    X_train_vec = vectorizer.fit_transform(newsgroups_train.data)\n",
    "    X_test_vec = vectorizer.transform(newsgroups_test.data)\n",
    "    \n",
    "    print(f\"Dimensionalidad: {X_train_vec.shape[1]} características\")\n",
    "    \n",
    "    for model_config in model_configs:\n",
    "        model = model_config['model']\n",
    "        model_name = model_config['name']\n",
    "        \n",
    "        # Entrenar y evaluar\n",
    "        f1, _ = evaluate_model(model, X_train_vec, y_train, X_test_vec, y_test, model_name)\n",
    "        \n",
    "        # Guardar resultado\n",
    "        result = {\n",
    "            'vectorizer': vec_config['name'],\n",
    "            'model': model_name,\n",
    "            'f1_score': f1,\n",
    "            'vectorizer_params': vec_config['params'],\n",
    "            'model_obj': model_config['model']\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # Actualizar mejor resultado\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_config = {\n",
    "                'vectorizer': vectorizer,\n",
    "                'model': model_config['model'],\n",
    "                'vectorizer_name': vec_config['name'],\n",
    "                'model_name': model_name,\n",
    "                'f1_score': f1\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESUMEN DE RESULTADOS:\n",
      "================================================================================\n",
      "TOP 10 MEJORES CONFIGURACIONES:\n",
      "--------------------------------------------------\n",
      "1. F1-score: 0.6954\n",
      "   Vectorizador: TF-IDF básico\n",
      "   Modelo: ComplementNB (alpha=0.1)\n",
      "\n",
      "2. F1-score: 0.6950\n",
      "   Vectorizador: TF-IDF con max_df=0.8\n",
      "   Modelo: ComplementNB (alpha=0.1)\n",
      "\n",
      "3. F1-score: 0.6936\n",
      "   Vectorizador: TF-IDF con stop_words=english\n",
      "   Modelo: ComplementNB (alpha=1.0)\n",
      "\n",
      "4. F1-score: 0.6935\n",
      "   Vectorizador: TF-IDF con min_df=2\n",
      "   Modelo: ComplementNB (alpha=1.0)\n",
      "\n",
      "5. F1-score: 0.6930\n",
      "   Vectorizador: TF-IDF básico\n",
      "   Modelo: ComplementNB (alpha=1.0)\n",
      "\n",
      "6. F1-score: 0.6925\n",
      "   Vectorizador: TF-IDF con min_df=2, max_df=0.8\n",
      "   Modelo: ComplementNB (alpha=1.0)\n",
      "\n",
      "7. F1-score: 0.6920\n",
      "   Vectorizador: TF-IDF con max_df=0.8\n",
      "   Modelo: ComplementNB (alpha=1.0)\n",
      "\n",
      "8. F1-score: 0.6919\n",
      "   Vectorizador: TF-IDF con stop_words=english\n",
      "   Modelo: ComplementNB (alpha=0.1)\n",
      "\n",
      "9. F1-score: 0.6907\n",
      "   Vectorizador: TF-IDF con min_df=2, max_df=0.8\n",
      "   Modelo: ComplementNB (alpha=0.1)\n",
      "\n",
      "10. F1-score: 0.6906\n",
      "   Vectorizador: TF-IDF con min_df=2\n",
      "   Modelo: ComplementNB (alpha=0.1)\n",
      "\n",
      "MEJOR CONFIGURACIÓN ENCONTRADA:\n",
      "----------------------------------------\n",
      "F1-score macro: 0.6954\n",
      "Vectorizador: TF-IDF básico\n",
      "Modelo: ComplementNB (alpha=0.1)\n",
      "\n",
      "ENTRENANDO MODELO FINAL CON LA MEJOR CONFIGURACIÓN:\n",
      "--------------------------------------------------\n",
      "F1-score macro final: 0.6954\n",
      "Número de características del mejor vectorizador: 101631\n",
      "\n",
      "REPORTE DE CLASIFICACIÓN DETALLADO:\n",
      "--------------------------------------------------\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.33      0.47      0.39       319\n",
      "           comp.graphics       0.72      0.72      0.72       389\n",
      " comp.os.ms-windows.misc       0.71      0.55      0.62       394\n",
      "comp.sys.ibm.pc.hardware       0.64      0.69      0.66       392\n",
      "   comp.sys.mac.hardware       0.77      0.71      0.74       385\n",
      "          comp.windows.x       0.80      0.79      0.79       395\n",
      "            misc.forsale       0.74      0.72      0.73       390\n",
      "               rec.autos       0.79      0.74      0.77       396\n",
      "         rec.motorcycles       0.81      0.77      0.79       398\n",
      "      rec.sport.baseball       0.90      0.84      0.87       397\n",
      "        rec.sport.hockey       0.86      0.95      0.90       399\n",
      "               sci.crypt       0.76      0.80      0.78       396\n",
      "         sci.electronics       0.71      0.57      0.63       393\n",
      "                 sci.med       0.76      0.79      0.77       396\n",
      "               sci.space       0.77      0.80      0.79       394\n",
      "  soc.religion.christian       0.57      0.86      0.69       398\n",
      "      talk.politics.guns       0.61      0.71      0.65       364\n",
      "   talk.politics.mideast       0.78      0.84      0.81       376\n",
      "      talk.politics.misc       0.66      0.43      0.52       310\n",
      "      talk.religion.misc       0.54      0.20      0.29       251\n",
      "\n",
      "                accuracy                           0.71      7532\n",
      "               macro avg       0.71      0.70      0.70      7532\n",
      "            weighted avg       0.72      0.71      0.71      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN DE RESULTADOS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Mostrar top 10 mejores resultados\n",
    "results_sorted = sorted(results, key=lambda x: x['f1_score'], reverse=True)\n",
    "\n",
    "print(\"TOP 10 MEJORES CONFIGURACIONES:\")\n",
    "print(\"-\" * 50)\n",
    "for i, result in enumerate(results_sorted[:10], 1):\n",
    "    print(f\"{i}. F1-score: {result['f1_score']:.4f}\")\n",
    "    print(f\"   Vectorizador: {result['vectorizer']}\")\n",
    "    print(f\"   Modelo: {result['model']}\")\n",
    "    print()\n",
    "\n",
    "print(\"MEJOR CONFIGURACIÓN ENCONTRADA:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"F1-score macro: {best_config['f1_score']:.4f}\")\n",
    "print(f\"Vectorizador: {best_config['vectorizer_name']}\")\n",
    "print(f\"Modelo: {best_config['model_name']}\")\n",
    "print()\n",
    "\n",
    "# Entrenar el mejor modelo y mostrar métricas adicionales\n",
    "print(\"ENTRENANDO MODELO FINAL CON LA MEJOR CONFIGURACIÓN:\")\n",
    "print(\"-\" * 50)\n",
    "best_model = best_config['model']\n",
    "best_vectorizer = best_config['vectorizer']\n",
    "\n",
    "# Re-entrenar con la mejor configuración\n",
    "X_train_best = best_vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test_best = best_vectorizer.transform(newsgroups_test.data)\n",
    "best_model.fit(X_train_best, y_train)\n",
    "y_pred_best = best_model.predict(X_test_best)\n",
    "\n",
    "final_f1 = f1_score(y_test, y_pred_best, average='macro')\n",
    "print(f\"F1-score macro final: {final_f1:.4f}\")\n",
    "print(f\"Número de características del mejor vectorizador: {X_train_best.shape[1]}\")\n",
    "\n",
    "# Análisis por clase\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nREPORTE DE CLASIFICACIÓN DETALLADO:\")\n",
    "print(\"-\" * 50)\n",
    "print(classification_report(y_test, y_pred_best, target_names=newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 3: \n",
    "  Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación (f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial y ComplementNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "PUNTO 3: ANÁLISIS DE SIMILARIDAD ENTRE PALABRAS\n",
      "====================================================================================================\n",
      "Número total de términos en el vocabulario: 101631\n",
      "Forma de la matriz transpuesta (término-documento): (101631, 11314)\n",
      "Cada fila representa ahora un término/palabra y cada columna un documento\n",
      "\n",
      "PALABRAS SELECCIONADAS PARA ANÁLISIS:\n",
      "--------------------------------------------------\n",
      "- computer\n",
      "- government\n",
      "- medical\n",
      "- religion\n",
      "- sports\n",
      "\n",
      "================================================================================\n",
      "ANÁLISIS DE SIMILARIDAD PARA: 'computer'\n",
      "================================================================================\n",
      "Índice de la palabra 'computer': 28940\n",
      "Aparece en 446 documentos\n",
      "Valor TF-IDF máximo: 0.3716\n",
      "\n",
      "LAS 5 PALABRAS MÁS SIMILARES A 'computer':\n",
      "--------------------------------------------------\n",
      "1. 'decwriter'\n",
      "   Similaridad coseno: 0.1563\n",
      "   Aparece en: 1 documentos\n",
      "   TF-IDF máximo: 0.2512\n",
      "\n",
      "2. 'deluged'\n",
      "   Similaridad coseno: 0.1522\n",
      "   Aparece en: 1 documentos\n",
      "   TF-IDF máximo: 0.2446\n",
      "\n",
      "3. 'harkens'\n",
      "   Similaridad coseno: 0.1522\n",
      "   Aparece en: 1 documentos\n",
      "   TF-IDF máximo: 0.2446\n",
      "\n",
      "4. 'shopper'\n",
      "   Similaridad coseno: 0.1443\n",
      "   Aparece en: 9 documentos\n",
      "   TF-IDF máximo: 0.2360\n",
      "\n",
      "5. 'the'\n",
      "   Similaridad coseno: 0.1361\n",
      "   Aparece en: 9475 documentos\n",
      "   TF-IDF máximo: 0.4899\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ANÁLISIS DE SIMILARIDAD PARA: 'government'\n",
      "================================================================================\n",
      "Índice de la palabra 'government': 44066\n",
      "Aparece en 550 documentos\n",
      "Valor TF-IDF máximo: 0.3322\n",
      "\n",
      "LAS 5 PALABRAS MÁS SIMILARES A 'government':\n",
      "--------------------------------------------------\n",
      "1. 'the'\n",
      "   Similaridad coseno: 0.2410\n",
      "   Aparece en: 9475 documentos\n",
      "   TF-IDF máximo: 0.4899\n",
      "\n",
      "2. 'to'\n",
      "   Similaridad coseno: 0.2251\n",
      "   Aparece en: 8465 documentos\n",
      "   TF-IDF máximo: 0.4028\n",
      "\n",
      "3. 'of'\n",
      "   Similaridad coseno: 0.2227\n",
      "   Aparece en: 7674 documentos\n",
      "   TF-IDF máximo: 0.3915\n",
      "\n",
      "4. 'libertarian'\n",
      "   Similaridad coseno: 0.2199\n",
      "   Aparece en: 42 documentos\n",
      "   TF-IDF máximo: 0.2761\n",
      "\n",
      "5. 'encryption'\n",
      "   Similaridad coseno: 0.2174\n",
      "   Aparece en: 169 documentos\n",
      "   TF-IDF máximo: 0.4390\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ANÁLISIS DE SIMILARIDAD PARA: 'medical'\n",
      "================================================================================\n",
      "Índice de la palabra 'medical': 60703\n",
      "Aparece en 127 documentos\n",
      "Valor TF-IDF máximo: 0.3737\n",
      "\n",
      "LAS 5 PALABRAS MÁS SIMILARES A 'medical':\n",
      "--------------------------------------------------\n",
      "1. 'romano'\n",
      "   Similaridad coseno: 0.2823\n",
      "   Aparece en: 2 documentos\n",
      "   TF-IDF máximo: 0.1574\n",
      "\n",
      "2. 'hospitals'\n",
      "   Similaridad coseno: 0.2751\n",
      "   Aparece en: 26 documentos\n",
      "   TF-IDF máximo: 0.2300\n",
      "\n",
      "3. 'recuperation'\n",
      "   Similaridad coseno: 0.2682\n",
      "   Aparece en: 2 documentos\n",
      "   TF-IDF máximo: 0.1574\n",
      "\n",
      "4. 'providers'\n",
      "   Similaridad coseno: 0.2391\n",
      "   Aparece en: 10 documentos\n",
      "   TF-IDF máximo: 0.2180\n",
      "\n",
      "5. 'relelvant'\n",
      "   Similaridad coseno: 0.2278\n",
      "   Aparece en: 1 documentos\n",
      "   TF-IDF máximo: 0.2648\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ANÁLISIS DE SIMILARIDAD PARA: 'religion'\n",
      "================================================================================\n",
      "Índice de la palabra 'religion': 77274\n",
      "Aparece en 234 documentos\n",
      "Valor TF-IDF máximo: 0.4680\n",
      "\n",
      "LAS 5 PALABRAS MÁS SIMILARES A 'religion':\n",
      "--------------------------------------------------\n",
      "1. 'religious'\n",
      "   Similaridad coseno: 0.2451\n",
      "   Aparece en: 183 documentos\n",
      "   TF-IDF máximo: 0.3795\n",
      "\n",
      "2. 'religions'\n",
      "   Similaridad coseno: 0.2116\n",
      "   Aparece en: 78 documentos\n",
      "   TF-IDF máximo: 0.4330\n",
      "\n",
      "3. 'categorized'\n",
      "   Similaridad coseno: 0.2039\n",
      "   Aparece en: 7 documentos\n",
      "   TF-IDF máximo: 0.2343\n",
      "\n",
      "4. 'purpsoe'\n",
      "   Similaridad coseno: 0.2008\n",
      "   Aparece en: 1 documentos\n",
      "   TF-IDF máximo: 0.1189\n",
      "\n",
      "5. 'crusades'\n",
      "   Similaridad coseno: 0.1987\n",
      "   Aparece en: 9 documentos\n",
      "   TF-IDF máximo: 0.1981\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ANÁLISIS DE SIMILARIDAD PARA: 'sports'\n",
      "================================================================================\n",
      "Índice de la palabra 'sports': 84538\n",
      "Aparece en 67 documentos\n",
      "Valor TF-IDF máximo: 0.4433\n",
      "\n",
      "LAS 5 PALABRAS MÁS SIMILARES A 'sports':\n",
      "--------------------------------------------------\n",
      "1. 'wip'\n",
      "   Similaridad coseno: 0.3699\n",
      "   Aparece en: 6 documentos\n",
      "   TF-IDF máximo: 0.4866\n",
      "\n",
      "2. 'jockeys'\n",
      "   Similaridad coseno: 0.3599\n",
      "   Aparece en: 1 documentos\n",
      "   TF-IDF máximo: 0.1398\n",
      "\n",
      "3. 'pollute'\n",
      "   Similaridad coseno: 0.3599\n",
      "   Aparece en: 1 documentos\n",
      "   TF-IDF máximo: 0.1398\n",
      "\n",
      "4. 'rockin'\n",
      "   Similaridad coseno: 0.3599\n",
      "   Aparece en: 1 documentos\n",
      "   TF-IDF máximo: 0.1398\n",
      "\n",
      "5. 'admittdly'\n",
      "   Similaridad coseno: 0.2565\n",
      "   Aparece en: 1 documentos\n",
      "   TF-IDF máximo: 0.2492\n",
      "\n",
      "\n",
      "================================================================================\n",
      "MATRIZ DE SIMILARIDAD ENTRE LAS PALABRAS SELECCIONADAS\n",
      "================================================================================\n",
      "Matriz de similaridad coseno:\n",
      "----------------------------------------\n",
      "                computer  government     medical    religion      sports\n",
      "    computer      1.0000      0.0395      0.0024      0.0038      0.0009\n",
      "  government      0.0395      1.0000      0.0149      0.0437      0.0019\n",
      "     medical      0.0024      0.0149      1.0000      0.0006      0.0001\n",
      "    religion      0.0038      0.0437      0.0006      1.0000      0.0000\n",
      "      sports      0.0009      0.0019      0.0001      0.0000      1.0000\n",
      "\n",
      "PARES DE PALABRAS MÁS SIMILARES:\n",
      "----------------------------------------\n",
      "1. 'government' ↔ 'religion': 0.0437\n",
      "2. 'computer' ↔ 'government': 0.0395\n",
      "3. 'government' ↔ 'medical': 0.0149\n",
      "\n",
      "================================================================================\n",
      "INTERPRETACIÓN DE LOS RESULTADOS:\n",
      "================================================================================\n",
      "\n",
      "Los vectores de palabras obtenidos mediante la transposición de la matriz TF-IDF\n",
      "representan a cada palabra en el espacio de documentos. Esto significa que:\n",
      "\n",
      "1. Dos palabras son similares si tienden a aparecer en documentos similares\n",
      "2. La similaridad coseno mide qué tan frecuentemente las palabras co-ocurren\n",
      "   en los mismos tipos de documentos\n",
      "3. Palabras del mismo dominio temático (ej: tecnología, política) deberían\n",
      "   mostrar mayor similaridad\n",
      "4. Este enfoque captura relaciones semánticas basadas en el contexto de uso\n",
      "\n",
      "Este método es una forma simple de obtener embeddings de palabras basados\n",
      "en la distribución de documentos, similar en concepto a técnicas más\n",
      "sofisticadas como Word2Vec o GloVe.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PUNTO 3: ANÁLISIS DE SIMILARIDAD ENTRE PALABRAS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Usar el vectorizador original para obtener los nombres de las características\n",
    "feature_names = tfidfvect.get_feature_names_out()\n",
    "print(f\"Número total de términos en el vocabulario: {len(feature_names)}\")\n",
    "\n",
    "# Transponer la matriz documento-término para obtener matriz término-documento\n",
    "X_train_transposed = X_train.T  # Ahora cada fila es un término y cada columna es un documento\n",
    "print(f\"Forma de la matriz transpuesta (término-documento): {X_train_transposed.shape}\")\n",
    "print(\"Cada fila representa ahora un término/palabra y cada columna un documento\\n\")\n",
    "\n",
    "# Seleccionar palabras manualmente para el análisis\n",
    "# Elegimos palabras que sean interpretables y comunes en el dataset\n",
    "selected_words = [\n",
    "    'computer',    # Tecnología\n",
    "    'government',  # Política  \n",
    "    'medical',     # Medicina\n",
    "    'religion',    # Religión\n",
    "    'sports'       # Deportes\n",
    "]\n",
    "\n",
    "print(\"PALABRAS SELECCIONADAS PARA ANÁLISIS:\")\n",
    "print(\"-\" * 50)\n",
    "for word in selected_words:\n",
    "    print(f\"- {word}\")\n",
    "print()\n",
    "\n",
    "# Función para encontrar el índice de una palabra en el vocabulario\n",
    "def find_word_index(word, feature_names):\n",
    "    try:\n",
    "        return list(feature_names).index(word)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Función para encontrar palabras similares basándose en documentos que contienen palabras similares\n",
    "def find_similar_words_by_documents(word_index, X_transposed, feature_names, top_k=5):\n",
    "    # Obtener el vector del término (fila correspondiente en la matriz transpuesta)\n",
    "    word_vector = X_transposed[word_index]\n",
    "    \n",
    "    # Calcular similaridad coseno con todos los otros términos\n",
    "    similarities = cosine_similarity(word_vector, X_transposed).flatten()\n",
    "    \n",
    "    # Obtener los índices de los términos más similares (excluyendo la palabra misma)\n",
    "    # Tomamos top_k+1 porque el primero será la misma palabra\n",
    "    most_similar_indices = similarities.argsort()[-(top_k+1):][::-1]\n",
    "    \n",
    "    # Remover la primera (la misma palabra)\n",
    "    most_similar_indices = most_similar_indices[1:]\n",
    "    \n",
    "    return most_similar_indices, similarities\n",
    "\n",
    "# Analizar cada palabra seleccionada\n",
    "for word in selected_words:\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ANÁLISIS DE SIMILARIDAD PARA: '{word}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Encontrar el índice de la palabra\n",
    "    word_index = find_word_index(word, feature_names)\n",
    "    \n",
    "    if word_index is None:\n",
    "        print(f\"La palabra '{word}' no se encuentra en el vocabulario.\")\n",
    "        print(\"Buscando palabras similares en el vocabulario...\")\n",
    "        \n",
    "        # Buscar palabras que contengan la palabra como substring\n",
    "        similar_in_vocab = [w for w in feature_names if word in w.lower()]\n",
    "        if similar_in_vocab:\n",
    "            print(f\"Palabras relacionadas encontradas: {similar_in_vocab[:10]}\")\n",
    "            # Usar la primera palabra encontrada\n",
    "            word = similar_in_vocab[0]\n",
    "            word_index = find_word_index(word, feature_names)\n",
    "            print(f\"Usando '{word}' para el análisis.\\n\")\n",
    "        else:\n",
    "            print(f\"No se encontraron palabras relacionadas con '{word}' en el vocabulario.\\n\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Índice de la palabra '{word}': {word_index}\")\n",
    "    \n",
    "    # Obtener información sobre la frecuencia de la palabra\n",
    "    word_vector = X_train_transposed[word_index]\n",
    "    total_docs_with_word = (word_vector > 0).sum()\n",
    "    max_tfidf = word_vector.max()\n",
    "    \n",
    "    print(f\"Aparece en {total_docs_with_word} documentos\")\n",
    "    print(f\"Valor TF-IDF máximo: {max_tfidf:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Encontrar palabras similares\n",
    "    similar_indices, similarities = find_similar_words_by_documents(\n",
    "        word_index, X_train_transposed, feature_names, top_k=5\n",
    "    )\n",
    "    \n",
    "    print(f\"LAS 5 PALABRAS MÁS SIMILARES A '{word}':\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for rank, similar_index in enumerate(similar_indices, 1):\n",
    "        similar_word = feature_names[similar_index]\n",
    "        similarity_score = similarities[similar_index]\n",
    "        \n",
    "        # Información adicional sobre la palabra similar\n",
    "        similar_vector = X_train_transposed[similar_index]\n",
    "        docs_with_similar = (similar_vector > 0).sum()\n",
    "        max_tfidf_similar = similar_vector.max()\n",
    "        \n",
    "        print(f\"{rank}. '{similar_word}'\")\n",
    "        print(f\"   Similaridad coseno: {similarity_score:.4f}\")\n",
    "        print(f\"   Aparece en: {docs_with_similar} documentos\")\n",
    "        print(f\"   TF-IDF máximo: {max_tfidf_similar:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Análisis adicional: Matriz de similaridad entre las palabras seleccionadas\n",
    "print(\"=\"*80)\n",
    "print(\"MATRIZ DE SIMILARIDAD ENTRE LAS PALABRAS SELECCIONADAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Obtener los índices de todas las palabras que pudimos encontrar\n",
    "valid_words = []\n",
    "valid_indices = []\n",
    "\n",
    "for word in selected_words:\n",
    "    word_index = find_word_index(word, feature_names)\n",
    "    if word_index is None:\n",
    "        # Buscar alternativas\n",
    "        similar_in_vocab = [w for w in feature_names if word in w.lower()]\n",
    "        if similar_in_vocab:\n",
    "            word = similar_in_vocab[0]\n",
    "            word_index = find_word_index(word, feature_names)\n",
    "    \n",
    "    if word_index is not None:\n",
    "        valid_words.append(word)\n",
    "        valid_indices.append(word_index)\n",
    "\n",
    "if len(valid_words) > 1:\n",
    "    # Crear matriz de vectores de las palabras válidas\n",
    "    word_vectors = X_train_transposed[valid_indices]\n",
    "    \n",
    "    # Calcular matriz de similaridad\n",
    "    similarity_matrix = cosine_similarity(word_vectors)\n",
    "    \n",
    "    print(\"Matriz de similaridad coseno:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Imprimir header\n",
    "    print(f\"{'':>12}\", end=\"\")\n",
    "    for word in valid_words:\n",
    "        print(f\"{word:>12}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    # Imprimir matriz\n",
    "    for i, word_i in enumerate(valid_words):\n",
    "        print(f\"{word_i:>12}\", end=\"\")\n",
    "        for j, word_j in enumerate(valid_words):\n",
    "            print(f\"{similarity_matrix[i][j]:>12.4f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Encontrar los pares más similares (excluyendo la diagonal)\n",
    "    print(\"PARES DE PALABRAS MÁS SIMILARES:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    pairs_similarity = []\n",
    "    for i in range(len(valid_words)):\n",
    "        for j in range(i+1, len(valid_words)):\n",
    "            pairs_similarity.append({\n",
    "                'word1': valid_words[i],\n",
    "                'word2': valid_words[j], \n",
    "                'similarity': similarity_matrix[i][j]\n",
    "            })\n",
    "    \n",
    "    # Ordenar por similaridad\n",
    "    pairs_similarity.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    for i, pair in enumerate(pairs_similarity[:3], 1):\n",
    "        print(f\"{i}. '{pair['word1']}' ↔ '{pair['word2']}': {pair['similarity']:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"No se encontraron suficientes palabras válidas para crear la matriz de similaridad.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETACIÓN DE LOS RESULTADOS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "Los vectores de palabras obtenidos mediante la transposición de la matriz TF-IDF\n",
    "representan a cada palabra en el espacio de documentos. Esto significa que:\n",
    "\n",
    "1. Dos palabras son similares si tienden a aparecer en documentos similares\n",
    "2. La similaridad coseno mide qué tan frecuentemente las palabras co-ocurren\n",
    "   en los mismos tipos de documentos\n",
    "3. Palabras del mismo dominio temático (ej: tecnología, política) deberían\n",
    "   mostrar mayor similaridad\n",
    "4. Este enfoque captura relaciones semánticas basadas en el contexto de uso\n",
    "\n",
    "Este método es una forma simple de obtener embeddings de palabras basados\n",
    "en la distribución de documentos, similar en concepto a técnicas más\n",
    "sofisticadas como Word2Vec o GloVe.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
